{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00628e7d-e5cd-4273-a44b-b91e00659469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform Bronze to Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afe01e7-a351-488e-9d7d-e07a3a341bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Define the container path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8a034d-f4c3-4e63-b12c-cf01fbe04f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "bronze_path = \"dbfs:/mnt/medallion/bronze/\"\n",
    "silver_path = \"dbfs:/mnt/medallion/silver/\"\n",
    "checkpoints_path = \"dbfs:/mnt/stream_checkpoints/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3edd0a54-7cf3-443e-b420-926d18ca918d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Initiate Autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabc7fa7-f6ab-44dd-8835-c52b4c711b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the streaming data from the bronze directory\n",
    "bronze_df = spark.readStream\\\n",
    "        .format(\"delta\")\\\n",
    "        .load(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfb687c-e707-4c79-af68-331e67be6baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the streaming data (optional)\n",
    "# display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d59d48fc-7365-4f7d-bf4b-0fc9d6c250bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Silver Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f7956a-e1fa-4a85-b777-0653f6f85b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A. Standarized Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927c4654-387b-4082-ad0f-e59d78ed996a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def correct_naming_conventions(df):\n",
    "    # Function to standardize column names\n",
    "    def standardize_column_name(col_name):\n",
    "        return col_name.lower().replace(\" \", \"_\")\n",
    "\n",
    "    # Apply the standardization function to all column names\n",
    "    new_column_names = [standardize_column_name(col_name) for col_name in df.columns]\n",
    "\n",
    "    # Rename the columns in the DataFrame\n",
    "    df = df.toDF(*new_column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc17ef3c-ba28-4505-9637-a6e287943d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standardized_df = correct_naming_conventions(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fc25380-1a77-4c12-9bf0-63d606a16031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B. Handling Null and Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01a10ff-9543-44e4-95b1-1e9b5235252f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handling Null and Duplicate Function\n",
    "def handling_nulls_and_duplicates(df):\n",
    "    print(\"Handling nulls and duplicates: \", end=\"\")\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_no_dup = df.dropDuplicates()\n",
    "\n",
    "    # Separate string and numeric columns\n",
    "    string_columns = [col for col, dtype in df.dtypes if dtype == \"string\"]\n",
    "    numeric_columns = [col for col, dtype in df.dtypes if dtype in [\"int\", \"double\", \"float\"]]\n",
    "\n",
    "    # Fill nulls for string columns with 'Unknown'\n",
    "    df_string = df_no_dup.fillna(\"Unknown\", subset=string_columns)\n",
    "\n",
    "    # Fill nulls for numeric columns with 0\n",
    "    df_clean = df_string.fillna(0, subset=numeric_columns)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a810e9ca-a3ed-4a0d-acbc-f18340311317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling nulls and duplicates: "
     ]
    }
   ],
   "source": [
    "cleaned_df = handling_nulls_and_duplicates(standardized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9614679-95c8-4fbc-859e-b635958d668f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C. Add 'daypart' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d74791-3230-4aa0-81f0-20190dabc763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the UDF to classify the daypart\n",
    "def classify_daypart(transaction_time):\n",
    "    time = datetime.strptime(transaction_time, '%H:%M:%S').time()\n",
    "    if time >= datetime.strptime('06:00:00', '%H:%M:%S').time() and time < datetime.strptime('12:00:00', '%H:%M:%S').time():\n",
    "        return 'Morning'\n",
    "    elif time >= datetime.strptime('12:00:00', '%H:%M:%S').time() and time < datetime.strptime('18:00:00', '%H:%M:%S').time():\n",
    "        return 'Afternoon'\n",
    "    elif time >= datetime.strptime('18:00:00', '%H:%M:%S').time() and time < datetime.strptime('22:00:00', '%H:%M:%S').time():\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "# Register the UDF\n",
    "classify_daypart_udf = udf(classify_daypart, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9723c37-a379-4589-9e59-f5a3988b2866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the UDF to create the 'daypart' column\n",
    "df_with_daypart = cleaned_df.withColumn('daypart', classify_daypart_udf(col('transaction_time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11386ab1-6de6-417e-9882-139d349d1870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the resulting DataFrame\n",
    "# display(df_with_daypart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1657be92-2dde-48d2-8b5f-010056ab24ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### D. Add 'transform_time' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8baebc0-54aa-4852-b22f-e42ad7092630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create 'transform_time' column\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df_silver = df_with_daypart.withColumn('transformed_time', sf.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "360ae051-aa64-4953-baa5-152c1901ee8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Save the silver data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a9e8df-aec7-44a6-9a23-0a0c7bc94d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the transformed data into silver\n",
    "silver = df_silver.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoints_path}/silver\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"SilverCoffeeStream\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.transformSilver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}